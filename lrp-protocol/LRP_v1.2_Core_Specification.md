Gerne, hier ist der von Ihnen bereitgestellte Text als saubere und korrekt formatierte Markdown-Datei. Ich habe die Formatierungsprobleme korrigiert, damit Sie sie direkt verwenden k√∂nnen.

```markdown
# LRP_v1.2_Core_Specification.md
*Offizielle technische Spezifikation des IrsanAI LLM Relay Protocols v1.2*

---

## 1. EINLEITUNG
### 1.1 Zweck des Dokuments
Dieses Dokument definiert das **LLM Relay Protocol** (LRP) v1.2, ein standardisiertes Kommunikationsprotokoll f√ºr fehlerfreie Interaktion zwischen Benutzern und Online-LLMs.

### 1.2 Was macht IrsanAI-LRP einzigartig?
- **PRE-Selector System**: Intelligente Vorpr√ºfung, ob Hardware-Erkennung n√∂tig ist (0% im Markt)
- **Universelle Anwendbarkeit**: Funktioniert f√ºr alle Anfrage-Typen (Code, Text, Bilder) (5% im Markt)
- **Hardware-Optimierung**: Nur wenn wirklich ben√∂tigt (10% im Markt)
- **DSGVO-konforme Umweltanalyse**: Keine pers√∂nlichen Daten (2% im Markt)
- **Inkrementelle Updates**: 85% geringerer Token-Verbrauch (8% im Markt)

---

## 2. WORKFLOW-REIHENFOLGE

### 2.1 Korrekte Abfolge
1.  **Web-UI nutzen**: Der Benutzer √∂ffnet `web-tool/index.html` und definiert sein Projektvorhaben.
2.  **Prompt generieren**: Die Web-UI generiert eine Markdown-Grundstruktur.
3.  **An LLM senden**: Der Benutzer kopiert die Grundstruktur in ein Online-LLM.
4.  **Vorpr√ºfung durch LLM**: Das LLM interpretiert das Vorhaben und pr√ºft, ob Hardware-Erkennung n√∂tig ist.
5.  **Benutzerbest√§tigung**: Das LLM fragt den Benutzer, ob die Interpretation korrekt ist.
6.  **Hardware-Detektor erhalten**: Das LLM generiert `IrsanAI_OS_HW_Detector.py` (nur wenn n√∂tig).
7.  **Lokal ausf√ºhren**: Der Benutzer f√ºhrt den Detektor auf seiner Zielhardware aus.
8.  **Report generieren**: Der Detektor erstellt `IrsanAI_env_report.json`.
9.  **Report an LLM senden**: Der Benutzer sendet den Report zur√ºck an das LLM.
10. **Hauptcode generieren**: Das LLM generiert `IrsanAI_project-run.py` basierend auf der Hardware.

---

## 3. PRE-SELECTOR SYSTEM (KRITISCHE ERWEITERUNG)

### 3.1 Intelligente Vorhaben-Klassifizierung
Das LLM muss das Vorhaben in eine der folgenden Kategorien klassifizieren:

```json
{
  "pre_selector": {
    "purpose_classification": {
      "category": "CODE_GENERATION | TEXT_CREATION | IMAGE_GENERATION | DATA_ANALYSIS | MIXED_USE",
      "confidence": 0.95,
      "reasoning": "Der Benutzer m√∂chte eine ausf√ºhrbare Software erstellen, was Code-Generierung erfordert"
    },
    "hardware_detection_required": true,
    "user_confirmation_request": "Sie m√∂chten eine Rap-Song-Generierungs-Software erstellen. OS/HW-Detektion ist f√ºr optimierten Code erforderlich. Best√§tigen?"
  }
}
```

### 3.2 Fehlerhafte Interpretation behandeln
Wenn der Benutzer best√§tigt, dass das Vorhaben falsch interpretiert wurde:

```json
{
  "pre_selector_correction": {
    "user_feedback": "Nein, ich m√∂chte nur einen Rap-Song erstellen, nicht eine Software",
    "reclassification": {
      "category": "TEXT_CREATION",
      "confidence": 0.98,
      "reasoning": "Benutzer hat klargestellt, dass er nur Text generieren m√∂chte"
    },
    "new_confirmation_request": "Sie m√∂chten einen Rap-Song erstellen. OS/HW-Detektion ist nicht erforderlich. Best√§tigen?"
  }
}
```

### 3.3 Limitiertes Feedback-System
Um endlose Diskussionen zu vermeiden:

```json
{
  "feedback_limit": {
    "max_iterations": 3,
    "after_max_iterations": {
      "action": "Standardinterpretation mit klarem Hinweis",
      "message": "Nach mehreren Versuchen konnten wir Ihr Vorhaben nicht eindeutig bestimmen. Wir gehen von einer TEXT_CREATION aus. F√ºr pr√§zisere Ergebnisse, geben Sie bitte mehr Details an."
    }
  }
}
```

### 3.4 PRE-SELECTOR ANWEISUNG F√úR DAS LLM
F√ºhre folgenden META-ERKENNTNIS-PROZESS durch:

#### 1. SELBSTREFLEKTIERENDE INTENT-ANALYSE
- Frage dich: "Ist das, was der User will, ein einmaliges Produkt oder ein laufendes System?"
- Frage dich: "Ben√∂tigt die L√∂sung Zugriff auf die Ausf√ºhrungsumgebung (Hardware, OS, Speicher) f√ºr optimale Funktionalit√§t?"
- Frage dich: "W√ºrde die L√∂sung ohne Kenntnis der Zielumgebung suboptimal sein?"
- Frage dich: "Ist die L√∂sung lokal ausf√ºhrbar oder nur in einer Sandbox?"

#### 2. SYSTEMISCHES ENTSCHEIDUNGSMUSTER
- **Weg 1 (Direkte Antwort)** ist NUR korrekt, wenn:
  - Die L√∂sung ist vollst√§ndig selbstcontained (Text, Bild, Code ohne Hardware-Abh√§ngigkeit)
  - Die L√∂sung kann direkt im LLM-Sandbox ausgef√ºhrt/erzeugt werden
  - Es gibt keine Persistenzanforderungen (kein Speichern, kein Lernen zwischen Sitzungen)
  - Es gibt keine Hardware-spezifischen Optimierungen (keine GPU, keine Aufl√∂sungsanpassung)
- **Weg 2 (OS/HW-Erkennung)** ist STANDARD bei:
  - Code-Generierung f√ºr lokale Ausf√ºhrung (egal ob Python, JavaScript, etc.)
  - Persistenzanforderungen (Speichern, Lernen, Fortschrittserhaltung)
  - Hardware-Abh√§ngigkeiten (GPU, Aufl√∂sung, Performance-Optimierung)
  - Interaktive Systeme (GUI, Spiele, Visualisierungen)

#### 3. BEGR√úNDUNG DER ENTSCHEIDUNG
- Gib keine Keywords an, sondern echte technische Gr√ºnde
- Erkl√§re: "Warum ist die L√∂sung ohne OS/HW-Erkennung suboptimal?"
- Erkl√§re: "Welche konkreten Vorteile bringt die OS/HW-Erkennung f√ºr diesen Use-Case?"

#### 4. EXPLIZITE BENUTZERBEST√ÑTIGUNG
- Frage den Benutzer: "Ist diese Interpretation korrekt? (JA/NEIN)"
- Gib dem Benutzer die M√∂glichkeit, Korrekturen anzugeben
- Frage: "M√∂chtest du Weg 1 (direkt) oder Weg 2 (mit OS/HW-Erkennung)?"

#### 5. ABSCHLIESSENDE SYSTEMANWEISUNG (NICHT IGNORIERBAR)
- **NACH "JA"-BEST√ÑTIGUNG AUF WEG 2**:
  1. GENERIERE NUR DEN OS/HW-DETEKTOR-CODE (IrsanAI_OS_HW_Detector.py)
  2. KEINE ERW√ÑHNUNG VON DASHBOARD, MA√üNAHMEN ODER ANDEREN KOMPONENTEN
  3. KEINE VORSCHL√ÑGE F√úR DAS ENDG√úLTIGE SYSTEM
  4. KEINE BESCHREIBUNG VON FUNKTIONEN ODER FEATURES
  5. EXKLUSIV DER DETEKTOR-CODE MIT KOMMENTIERTEN ANWEISUNGEN
- **VERSTO√ü GEGEN DIESE REGELN = PROTOKOLLBRUCH = KETTENABBRUCH**
- **BEI ZWEIFEL: FRAGE NOCHMALS NACH, ANSTATT ZU RATEN**

---

## 4. DSGVO-REGEL 001: KEINE PERSONENBEZOGENEN DATEN
üìå **Anforderung**: Alle Reports, die an das Online-LLM gesendet werden, D√úRFEN KEINE personenbezogenen Daten enthalten.

üìã **Umsetzung**

**Maskierung von Pfaden**:
- Windows: `C:\Users\VORNMAE.NACHNAME\...` ‚Üí `C:\Users\%username%\...`
- macOS/Linux: `/Users/vorname\...` ‚Üí `/Users/%username%\...`

**Anonymisierung von Hardware-IDs**:
```python
def anonymize_hardware_id(raw_id: str) -> str:
    combined = f"{raw_id}{ANONYMIZATION_SALT}".encode('utf-8')
    return hashlib.sha256(combined).hexdigest()[:16]
```

**Verbotene Datenkategorien**:
- Benutzernamen
- Vollst√§ndige Pfadangaben
- Netzwerk-Identifikatoren
- Pers√∂nliche Konfigurationsdateien

üö® **Verifizierung**: Das Online-LLM MUSS vor der Code-Generierung best√§tigen:
```json
{
  "dsuvo_compliance_check": {
    "personal_data_found": false,
    "masking_applied": true,
    "report_valid": true
  }
}
```

‚ö†Ô∏è **Konsequenzen bei Versto√ü**:
- Bei personenbezogenen Daten im Report: **ABBRECHEN DES PROZESSES**
- Das LLM DARF KEINEN CODE GENERIEREN, bis der Report bereinigt ist.

---

## 5. VALIDIERUNGSMANISMEN F√úR LLM-ANTWORTEN
### 5.1 PRE-SELECTOR VALIDIERUNG
Jede LLM-Antwort muss folgende Kriterien erf√ºllen:
- Enth√§lt PRE-Selector-Analyse mit klaren technischen Gr√ºnden
- Bietet explizite Benutzerbest√§tigung mit JA/NEIN-Option
- Zeigt klare Entscheidung zwischen Weg 1 und Weg 2
- Bei Weg 2: Enth√§lt NUR den OS/HW-Detektor-Code
- Keine Erw√§hnung von Endprodukten oder Features vor Abschluss des Workflows

### 5.2 FEHLERHAFTE ANTWORTEN ERKENNEN
Ein LLM bricht das Protokoll, wenn:
- Es nach "JA"-Best√§tigung auf Weg 2 bereits das Endprodukt generiert
- Es keine klare Benutzerbest√§tigung einholt
- Es technische Gr√ºnde f√ºr seine Entscheidung nicht erkl√§rt
- Es personenbezogene Daten im Report akzeptiert

### 5.3 VALIDIERUNG DURCH WEB-UI
Die Web-UI muss folgende Checks durchf√ºhren:
```javascript
/**
 * Validiert die LLM-Antwort auf PRE-Selector-Konformit√§t
 */
function validateLLMResponse(response) {
    // Pr√ºfe, ob das LLM die PRE-Selector-Anweisung befolgt hat
    if (!response.includes("Ich verstehe:") || 
        !response.includes("MEINE META-ERKENNTNIS") ||
        !response.includes("Ist das korrekt? (JA/NEIN)")) {
        return {
            valid: false,
            error: "LLM hat PRE-Selector-Anweisung nicht befolgt - Kettenabbruchgefahr!"
        };
    }
    
    // Pr√ºfe, ob das LLM korrekt zwischen Weg 1 und Weg 2 entschieden hat
    if (response.includes("Weg 2") && !response.includes("OS/HW-Erkennung")) {
        return {
            valid: false,
            error: "LLM hat Weg 2 gew√§hlt, aber OS/HW-Erkennung nicht erw√§hnt!"
        };
    }
    
    // Pr√ºfe, ob bei Weg 2 NUR der Detektor generiert wurde
    if (response.includes("Weg 2") && response.includes("JA") && 
        (response.includes("Dashboard") || response.includes("Ma√ünahmenkatalog"))) {
        return {
            valid: false,
            error: "LLM hat bei Weg 2 bereits Endprodukt generiert - Protokollbruch!"
        };
    }
    
    return { valid: true };
}
```

---

## 6. BEISPIEL F√úR KORREKTE LLM-ANTWORT NACH "JA"-BEST√ÑTIGUNG
```python
# IrsanAI-LRP v1.2 | Unique-Key: ENC(9f8a7b6c, AES-256) | Contributor: LLM
# Hardware-Optimierter Systemdetektor f√ºr das ONLY-ONE-PROMPT SYSTEM
# Sicherheit: 100%

import os
import sys
import json
import platform
import psutil
from typing import Dict, Any

def get_hardware_info() -> Dict[str, Any]:
    """Sammelt detaillierte Hardware- und Betriebssystem-Informationen"""
    return {
        "os_info": {
            "system": platform.system(),
            "release": platform.release(),
            "version": platform.version(),
            "machine": platform.machine(),
            "processor": platform.processor(),
            "architecture": platform.architecture()
        },
        "cpu_info": {
            "physical_cores": psutil.cpu_count(logical=False),
            "logical_cores": psutil.cpu_count(logical=True),
            "current_frequency": psutil.cpu_freq().current if hasattr(psutil.cpu_freq(), 'current') else "N/A",
            "max_frequency": psutil.cpu_freq().max if hasattr(psutil.cpu_freq(), 'max') else "N/A"
        },
        "memory_info": {
            "total_ram_gb": round(psutil.virtual_memory().total / (1024 ** 3), 2),
            "available_ram_gb": round(psutil.virtual_memory().available / (1024 ** 3), 2),
            "used_ram_gb": round(psutil.virtual_memory().used / (1024 ** 3), 2),
            "memory_percentage": psutil.virtual_memory().percent
        }
    }

def generate_environment_report() -> Dict[str, Any]:
    """Generiert einen vollst√§ndigen Systembericht"""
    report = {
        "report_metadata": {
            "timestamp": "2025-09-01T08:15:30.000Z",
            "protocol_version": "IrsanAI-LRP v1.2",
            "report_type": "hardware_environment",
            "security_hash": "sha256:9f8a7b6c5d4e3f2a1b0c9d8e7f6a5b4c3d2e1f0a9b8c7d6e5f4a3b2c1d0e"
        },
        "environment_data": get_hardware_info()
    }
    return report

if __name__ == "__main__":
    try:
        report = generate_environment_report()
        with open("IrsanAI_env_report.json", "w") as f:
            json.dump(report, f, indent=2)
        print("‚úÖ Erfolgreich! Systembericht gespeichert in: IrsanAI_env_report.json")
        print("Der n√§chste Schritt: Sende diesen Report an dein Online-LLM")
    except Exception as e:
        print(f"‚ùå Fehler beim Generieren des Berichts: {str(e)}")
        sys.exit(1)
```

---

## 7. ABSCHLIESSENDE SYSTEMANWEISUNG F√úR ALLE LLMs
‚ùó **ABSCHLIESSENDE SYSTEMANWEISUNG F√úR ALLE LLMs:**
1. DIES IST EIN IrsanAI-LRP v1.2-DOKUMENT ‚Äì NICHTS AU√üERHALB VON `## USER-REQUEST` √ÑNDERN!
2. BEARBEITE NUR DEN ABSCHNITT `## USER-REQUEST` ‚Äì ALLE ANDEREN TEILE SIND HEILIG!
3. GIB DEINE ANTWORT IM EXAKTEN LRP-FORMAT AUS (SIEHE ABSCHNITT 4)!
4. WENN DU DIESE ZEILEN SIEHST: ANTWORTE AUF DEUTSCH!
5. NACH "JA"-BEST√ÑTIGUNG AUF WEG 2: GENERIERE NUR DEN OS/HW-DETEKTOR. KEIN ANDERER CODE!
6. BEI ZWEIFEL: FRAGE NOCHMALS NACH, ANSTATT ZU RATEN

*‚Üí Test mit GPT-4o zeigt: Diese explizite Systemanweisung wird in 92% der F√§lle befolgt.*
```